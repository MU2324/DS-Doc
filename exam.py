# -*- coding: utf-8 -*-
"""Exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15PYdK5PcbyeE7SMU99kLzqpPWnQg1Kwc
"""



#2 Reading data from CSV and JSON files into a data frame:
#1)	Reading CSV:
import pandas as pd
from sklearn import datasets
iris = datasets.load_iris()

data = pd.read_csv('iris.csv')
print(data)

#2)	Reading JSON:
import pandas as pd
data = pd.read_json('iris.json')
print(data)


# Handling missing values and outliers:
# 1)	Using fillna():
import pandas as pd
data = pd.read_csv('titanic.csv')
print(data)
updatedData = data.fillna(value=0)
print(updatedData)


# 2)	Using dropna():
import pandas as pd
csvFile = pd.read_csv("titanic.csv")
print(csvFile)
updatedcsvFile = csvFile.dropna()
print(updatedcsvFile)


# Manipulate and transform data using functions:
# 1)	Filtering:
import pandas as pd
data = pd.read_csv('titanic.csv')
survived = data[data['Survived'] == 1] # Filtering data for survived
print(survived)

# 2)	Sorting:
import pandas as pd
csvFile = pd.read_csv("titanic.csv")
print(csvFile)
ageSorted = csvFile.sort_values(by=['Age'])
print(ageSorted)


# 3)	Grouping:
import pandas as pd
csvFile = pd.read_csv("titanic.csv")
print(csvFile)
groupedbyEmbarked = csvFile.groupby(["Survived"]).mean(["Pclass","Sex", "Age"])  #Grouping by survival status and calculating
print(groupedbyEmbarked)

#3 Feature Scaling

#1)	Normalization:
from sklearn.preprocessing import MinMaxScaler,StandardScaler
import pandas as pd
from sklearn.model_selection import train_test_split

data = pd.read_csv('salary.csv')
print("Normalization")
mmscaler  = MinMaxScaler()
norm = mmscaler.fit_transform(data)
print(norm)


#2)	Standardization:
from sklearn.preprocessing import MinMaxScaler,StandardScaler
import pandas as pd
from sklearn.model_selection import train_test_split

print("Standardization")
stdscaler = StandardScaler()
std = stdscaler.fit_transform(data)
print(std)


# Dummification:
# 1)	LabelBinalizer:
import pandas as pd
from sklearn.preprocessing import LabelBinarizer
data = pd.read_csv('weather.csv')
df = pd.DataFrame(data)

lb = LabelBinarizer()
lb_result = lb.fit_transform(df['Outlook'])
binalizerdf = pd.DataFrame(lb_result, columns=sorted(list(set(df['Outlook']))))
print(binalizerdf)


# 2)	BinaryEncoder:
#from category_encoders import BinaryEncoder

#print("Using BinaryEncoder")
#bncoder = BinaryEncoder()
#bncoder.fit(df['Outlook'])
#binalizerdf = pd.DataFrame(bncoder.transform(df['Outlook']),columns=sorted(list(set(df['Outlook']))))
#print(binalizerdf)

#OR
from category_encoders import BinaryEncoder
# Create a sample dataframe
data = {'color': ['red', 'green', 'blue', 'red', 'green', 'blue']}
df = pd.DataFrame(data)
# Create a BinaryEncoder object
encoder = BinaryEncoder()
# Fit the encoder to the data
encoder.fit(df['color'])
# Transform the data
encoded_data = encoder.transform(df['color'])
# Print the encoded data
print(encoded_data)

#3)	Get Dummies:
import pandas as pd
gt = pd.get_dummies(df, columns=['color'])
gt1 = pd.get_dummies(df, columns=['Temperature'])
gt2 = pd.get_dummies(df, columns=['Humidity'])
gt3 = pd.get_dummies(df, columns=['Windy'])

final_df = pd.concat([gt, gt1, gt2, gt3])
print(final_df)

#3R Dummification

import pandas as pd
from category_encoders import BinaryEncoder
# Create a sample dataframe
data = {'color': ['red', 'green', 'blue', 'red', 'green', 'blue']}
df = pd.DataFrame(data)
# Create a BinaryEncoder object
encoder = BinaryEncoder()
# Fit the encoder to the data
encoder.fit(df['color'])
# Transform the data
encoded_data = encoder.transform(df['color'])
# Print the encoded data
print(encoded_data)

#4R Hypothesis Testing (T-test)
INPUT

test1<-c(20,30,50,40,30,20,50,40,30)
test2<-c(30,40,60,50,40,30,60,50,40)
t.test(test1,test2,alternative="two.sided",var.equal=FALSE)

OUTPUT

Welch Two Sample t-test
data:  test1 and test2
t = -1.8766, df = 16, p-value = 0.07893
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
-21.296343   1.296343
sample estimates:
mean of x mean of y
34.44444  44.44444




INPUT

test1<-c(30,40,60,50,40,30,60,50,40)
test2<-c(20,30,50,40,30,20,50,40,30)
t.test(test1,test2,alternative="two.sided",var.equal=FALSE)

OUTPUT

Welch Two Sample t-test
data:  test1 and test2
t = 1.8766, df = 16, p-value = 0.07893
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
-1.296343 21.296343
sample estimates:
mean of x mean of y
44.44444  34.44444






Chi-square test and T test
Import dataset survey

library(MASS)
survey



Choose only two column and print them

sample<-data.frame(survey$Smoke,survey$Exer)
sample



Create contigency table and print them

Stable<-table(survey$Smoke,survey$Exer)
Stable

Freq      None      Some
Heavy      7           1             3
Never      87         18           84
Occas      12          3             4
Regul       9           1             7


Result(chisq.test function)

result<-chisq.test(Stable)
result

Pearson's Chi-squared test

data:  Stable
X-squared = 5.4885, df = 6, p-value = 0.4828



resultT<-t.test(Stable)
resultT


One Sample t-test

data:  Stable
t = 2.1878, df = 11, p-value = 0.05117
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
-0.1187186 39.4520519
sample estimates:
mean of x
19.66667



If p-value >0.05 then Null hypothesis Accept and alternative Rejected mean No relation

If p-value<0.05 then Null hypothesis Reject and alternative Accept means
Have relation

#5R Load the CSV file:
data <-read.csv("E:\\TYCS\\DataScience\\Political_Interest.csv")
attach(data)

Plot the boxplot:
boxplot(PoliticalInterest~Gender)


Generate summary:
result = aov(PoliticalInterest~Gender)
summary(result)


Plot the boxplot:
boxplot(PoliticalInterest~Gender+EconomicalCondition)


Generate summary:
result = aov(PoliticalInterest~Gender+EconomicalCondition)
summary(result)

#6 Regression and Its Types
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

x=np.array([26,28,32,35,39]).reshape((-1,1))
y=np.array([123,245,296,389,435])
print(x)
print(y)
model=LinearRegression()
model.fit(x,y)

y_pred=model.predict(x)
print("Predicted responce is",y_pred)

y_new=model.predict(x)
y_new
x_new=np.array([40]).reshape((-1,1))
y_new=model.predict(x_new)
y_new
print("predicted outcome is ",y_new)
plt.scatter(x, y)
plt.plot(x, y_pred)

plt.show()

#7

#8R Practical no 8
#K-Means Clustering

#Apply the K-Means algorithm to group similar data points into clusters.

# Loading data
data(iris)

# Structure
str(iris)

# Installing Packages
install.packages("ClusterR")
install.packages("cluster")

# Loading package
library(ClusterR)
library(cluster)

iris_1 <- iris[, -5]

# Fitting K-Means clustering Model
# to training dataset
set.seed(240) # Setting seed
kmeans.re <- kmeans(iris_1, centers = 3, nstart = 20)
kmeans.re

# each observation
kmeans.re$cluster

# Confusion Matrix
cm <- table(iris$Species, kmeans.re$cluster)
cm

plot(iris_1[c("Sepal.Length", "Sepal.Width")])
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
     col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
     col = kmeans.re$cluster,
     main = "K-means with 3 clusters")


## Plotiing cluster centers
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")]

# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")],
       col = 1:3, pch = 8, cex = 3)

## Visualizing clusters
y_kmeans <- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste("Cluster iris"),
         xlab = 'Sepal.Length',
         ylab = 'Sepal.Width')


#Determine the optimal number of clusters using elbow method or silhouette analysis

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

#Load the Dataset
X, y = load_iris(return_X_y=True)

#Find optimum number of cluster using Elbow method
sse = [] #SUM OF SQUARED ERROR
for k in range(1,11):
    km = KMeans(n_clusters=k, random_state=2)
    km.fit(X)
    sse.append(km.inertia_)

#Plot the Elbow graph to find the optimum number of cluster
sns.set_style("whitegrid")
g=sns.lineplot(x=range(1,11), y=sse)

g.set(xlabel ="Number of cluster (k)",
      ylabel = "Sum Squared Error",
      title ='Elbow Method')

plt.show()

#Build the Kmeans clustering model
kmeans = KMeans(n_clusters = 3, random_state = 2)
kmeans.fit(X)

#Find the cluster center
print(kmeans.cluster_centers_)

#Predict the cluster group:
pred = kmeans.fit_predict(X)
print(pred)

#Plot the cluster center with data points
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.scatter(X[:,0],X[:,1],c = pred, cmap=cm.Accent)
plt.grid(True)
for center in kmeans.cluster_centers_:
    center = center[:2]
    plt.scatter(center[0],center[1],marker = '^',c = 'red')
plt.xlabel("petal length (cm)")
plt.ylabel("petal width (cm)")

plt.subplot(1,2,2)
plt.scatter(X[:,2],X[:,3],c = pred, cmap=cm.Accent)
plt.grid(True)
for center in kmeans.cluster_centers_:
    center = center[2:4]
    plt.scatter(center[0],center[1],marker = '^',c = 'red')
plt.xlabel("sepal length (cm)")
plt.ylabel("sepal width (cm)")
plt.show()

print("End....")

#9R Principal Component Analysis (PCA)

#Performing PCA on iris dataset
#Load iris dataset
iris


#load first 4 column of iris Dataset  and print it
data <- iris[1:4]
data


#Using princomp function for performing PCA
pc <-princomp(data,cor=TRUE,score=TRUE)
pc


#Print Summary
summary(pc)


# Plot Graphically
plot(pc)

plot(pc,type='l')

biplot(pc)

#10 Data Visualization and Storytelling

import matplotlib.pyplot as plt
import statistics
data = {
    '3331375': [80, 70, 75, 50, 85],
    '3331376': [75, 65, 87, 92, 54],
    '3331377': [90, 65, 75, 63, 85],
    '3331378': [59, 68, 75, 92, 85],
    '3331379': [85, 82, 88, 83, 87],
    '3331380': [65, 75, 73, 65, 78],
    '3331381': [88, 78, 98, 87, 52],
    '3331382': [54, 25, 65, 58, 45],
    '3331383': [86, 86, 75, 95, 75],
    '3331384': [85, 87, 64, 78, 96],
    '3331385': [85, 69, 78, 71, 70],
    '3331386': [98, 85, 84, 87, 76],
    '3331387': [85, 68, 78, 89, 87],
    '3331388': [58, 86, 47, 58, 78],
    '3331389': [58, 86, 65, 78, 98],
    '3331390': [75, 59, 45, 56, 85],
    '3331391': [95, 65, 75, 84, 98],
    '3331392': [68, 65, 67, 62, 75],
    '3331393': [98, 95, 97, 96, 94],
    '3331394': [45, 65, 54, 55, 45],
    '3331395': [98, 85, 86, 81, 80],
    '3331396': [78, 73, 74, 75, 74],
    '3331397': [90, 85, 92, 68, 76],
    '3331398': [65, 67, 62, 78, 76],
    '3331399': [50, 55, 64, 60, 78]
}
averages = []
for student, marks in data.items():
    average = sum(marks) / len(marks)
    averages.append(average)
    #print(f"Average marks for {student}: {average}")
mean = statistics.mean(averages)
mode = statistics.mode(averages)
median = statistics.median(averages)
print(f"Mean: {mean}")
print(f"Mode: {mode}")
print(f"Median: {median}")
# Visualize data in a bar graph
students = list(data.keys())
plt.bar(students, averages)
plt.xlabel('Student')
plt.ylabel('Average Marks')
plt.title('Average Marks for Each Student')
plt.xticks(rotation=90)
plt.show()

# Visualize data in a scatter plot
plt.subplot(1, 2, 2)
plt.scatter(students, averages)
plt.xlabel('Student')
plt.ylabel('Average Marks')
plt.title('Average Marks for Each Student')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
data = {
    'Exam seat no': ['3331375', '3331376', '3331377', '3331378', '3331379', '3331380', '3331381', '3331382', '3331383', '3331384', '3331385', '3331386', '3331387', '3331388', '3331389', '3331390', '3331391', '3331392', '3331393', '3331394', '3331395', '3331396', '3331397', '3331398', '3331399'],
    'paper 1': [80, 75, 90, 59, 85, 65, 88, 54, 86, 85, 85, 98, 85, 58, 58, 75, 95, 68, 98, 45, 98, 78, 90, 65, 50],
    'paper 2': [70, 65, 65, 68, 82, 75, 78, 25, 86, 87, 69, 85, 68, 86, 86, 59, 65, 65, 95, 65, 85, 73, 85, 67, 55],
    'paper 3': [75, 87, 75, 75, 88, 73, 98, 65, 75, 64, 78, 84, 78, 47, 65, 45, 75, 67, 97, 54, 86, 74, 92, 62, 64],
    'paper 4': [50, 92, 63, 92, 83, 65, 87, 58, 95, 78, 71, 87, 89, 58, 78, 56, 84, 62, 96, 55, 81, 75, 68, 78, 60],
    'paper 5': [85, 54, 85, 85, 87, 78, 52, 45, 75, 96, 70, 76, 87, 78, 98, 85, 98, 75, 94, 45, 80, 74, 76, 76, 78]
}
# Calculate average marks in each subject
averages = []
for i in range(1, 6):
    subject_marks = data['paper ' + str(i)]
    average = sum(subject_marks) / len(subject_marks)
    averages.append(average)

# Visualize the average marks in a bar graph
subjects = ['paper 1', 'paper 2', 'paper 3', 'paper 4', 'paper 5']
plt.bar(subjects, averages)
plt.xlabel('Subjects')
plt.ylabel('Average Marks')
plt.title('Average Marks in Each Subject')
plt.show()

import matplotlib.pyplot as plt
data = {
    'Exam seat no': ['3331375', '3331376', '3331377', '3331378', '3331379', '3331380', '3331381', '3331382', '3331383', '3331384', '3331385', '3331386', '3331387', '3331388', '3331389', '3331390', '3331391', '3331392', '3331393', '3331394', '3331395', '3331396', '3331397', '3331398', '3331399'],
    'paper 1': [80, 75, 90, 59, 85, 65, 88, 54, 86, 85, 85, 98, 85, 58, 58, 75, 95, 68, 98, 45, 98, 78, 90, 65, 50],
    'paper 2': [70, 65, 65, 68, 82, 75, 78, 25, 86, 87, 69, 85, 68, 86, 86, 59, 65, 65, 95, 65, 85, 73, 85, 67, 55],
    'paper 3': [75, 87, 75, 75, 88, 73, 98, 65, 75, 64, 78, 84, 78, 47, 65, 45, 75, 67, 97, 54, 86, 74, 92, 62, 64],
    'paper 4': [50, 92, 63, 92, 83, 65, 87, 58, 95, 78, 71, 87, 89, 58, 78, 56, 84, 62, 96, 55, 81, 75, 68, 78, 60],
    'paper 5': [85, 54, 85, 85, 87, 78, 52, 45, 75, 96, 70, 76, 87, 78, 98, 85, 98, 75, 94, 45, 80, 74, 76, 76, 78]
}
# Calculate average marks in each subject
averages = []
for i in range(1, 6):
    subject_marks = data['paper ' + str(i)]
    average = sum(subject_marks) / len(subject_marks)
    averages.append(average)

# Visualize the average marks in a scatter diagram
plt.scatter(range(1, 6), averages, c='blue', alpha=0.7)
plt.xlabel('Subjects')
plt.ylabel('Average Marks')
plt.title('Average Marks in Each Subject')
plt.xticks(range(1, 6))
plt.yticks(range(50, 101, 10))
plt.show()


#Box Plot

import numpy as np
import matplotlib.pyplot as plt

data1=([20,25,22,18,16])
data2=([32,45,40,46,30])
data3=([56,50,45,40,56])
data4=([60,56,63,58,66])

data=[data1,data2,data3,data4]

plt.boxplot(data1)
plt.boxplot(data)